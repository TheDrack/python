name: Jarvis Self-Healing Auto-Code ðŸ¤–

# Triggered by Jarvis API via repository_dispatch
# Receives intent (create/fix) and instruction from Jarvis
# Uses GitHub Copilot for native intelligence
# Tests changes and creates PR only if tests pass

on:
  repository_dispatch:
    types: [jarvis_order]

jobs:
  auto_code:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      issues: write

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure Git User
        run: |
          git config --global user.name "Jarvis-Auto-Code"
          git config --global user.email "jarvis-autocode[bot]@users.noreply.github.com"

      - name: Extract Jarvis Order
        id: order
        run: |
          echo "Intent: ${{ github.event.client_payload.intent }}"
          echo "Instruction: ${{ github.event.client_payload.instruction }}"
          echo "Context: ${{ github.event.client_payload.context }}"
          echo "Triggered by: ${{ github.event.client_payload.triggered_by }}"
          
          # Save to environment
          echo "JARVIS_INTENT=${{ github.event.client_payload.intent }}" >> $GITHUB_ENV
          echo "JARVIS_INSTRUCTION=${{ github.event.client_payload.instruction }}" >> $GITHUB_ENV
          echo "JARVIS_CONTEXT=${{ github.event.client_payload.context }}" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Cache Dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-python-${{ hashFiles('requirements.txt', 'requirements/*.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-

      - name: Install GitHub Copilot CLI
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh --version
          
          # Install GitHub Copilot CLI extension
          echo "Installing GitHub Copilot CLI extension..."
          gh extension install github/gh-copilot || echo "Copilot extension already installed"
          
          # Verify installation
          gh copilot --version || {
            echo "Failed to install GitHub Copilot CLI extension"
            exit 1
          }

      - name: Create Feature Branch
        run: |
          BRANCH_NAME="jarvis-auto-code-$(date +%s)"
          echo "BRANCH_NAME=$BRANCH_NAME" >> $GITHUB_ENV
          git checkout -b "$BRANCH_NAME"

      - name: Apply Code Changes with Copilot
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Build the prompt for Copilot based on intent
          if [ "$JARVIS_INTENT" = "create" ]; then
            PROMPT="Create the following: $JARVIS_INSTRUCTION"
          else
            PROMPT="Fix the following: $JARVIS_INSTRUCTION"
          fi
          
          if [ ! -z "$JARVIS_CONTEXT" ]; then
            PROMPT="$PROMPT. Context: $JARVIS_CONTEXT"
          fi
          
          echo "Asking Copilot to: $PROMPT"
          
          # NOTE: This is a placeholder implementation for the workflow structure.
          # In production, this should integrate with:
          # 1. GitHub Copilot API (when available for Actions) - Use official API to generate code
          # 2. Alternative LLM (e.g., GPT-4, Claude) - For more complex code generation tasks
          # 3. Custom code generation logic - Based on instruction parsing and templates
          #
          # The current implementation creates a marker file to demonstrate the workflow.
          # Future enhancement: Replace with actual code generation using gh copilot or LLM API
          python3 << 'PYTHON_SCRIPT'
import os
import subprocess
import sys

intent = os.getenv('JARVIS_INTENT')
instruction = os.getenv('JARVIS_INSTRUCTION')
context = os.getenv('JARVIS_CONTEXT', '')

# For now, we'll create a placeholder to show the workflow ran
# TODO: Integrate with GitHub Copilot API or LLM for actual code generation
print(f"Processing {intent} request: {instruction}")

# This is a placeholder - in real implementation:
# 1. Use gh copilot or GitHub API to get code suggestions
# 2. Apply changes to files based on instruction
# 3. For now, we'll create a marker file to show the workflow ran

marker_file = f".jarvis_order_{intent}_{os.getenv('GITHUB_RUN_ID')}.txt"
with open(marker_file, 'w') as f:
    f.write(f"Intent: {intent}\n")
    f.write(f"Instruction: {instruction}\n")
    f.write(f"Context: {context}\n")
    f.write(f"Status: Placeholder - Changes would be applied here using GitHub Copilot or LLM\n")

print(f"Created marker file: {marker_file}")
PYTHON_SCRIPT

      - name: Detect Test Command
        id: test_cmd
        run: |
          # Auto-detect test command based on repository structure
          if [ -f "pytest.ini" ] || [ -f "setup.cfg" ]; then
            echo "TEST_CMD=pytest tests/ -v" >> $GITHUB_ENV
            echo "Test framework: pytest"
          elif find requirements -name "*.txt" -exec grep -q pytest {} \; 2>/dev/null; then
            echo "TEST_CMD=pytest tests/ -v" >> $GITHUB_ENV
            echo "Test framework: pytest (from requirements)"
          elif grep -q pytest requirements.txt 2>/dev/null; then
            echo "TEST_CMD=pytest tests/ -v" >> $GITHUB_ENV
            echo "Test framework: pytest (from requirements.txt)"
          elif [ -f "package.json" ] && grep -q "\"test\":" package.json; then
            echo "TEST_CMD=npm test" >> $GITHUB_ENV
            echo "Test framework: npm"
          elif [ -f "Makefile" ] && grep -q "^test:" Makefile; then
            echo "TEST_CMD=make test" >> $GITHUB_ENV
            echo "Test framework: make"
          else
            echo "TEST_CMD=echo 'No tests found, skipping'" >> $GITHUB_ENV
            echo "No test framework detected"
          fi

      - name: Install Test Dependencies
        if: contains(env.TEST_CMD, 'pytest')
        run: |
          # Install dependencies for Python projects
          if [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
          fi
          
          # Install test dependencies
          if [ -f "requirements/dev.txt" ]; then
            pip install -r requirements/dev.txt
          fi
          
          # Ensure pytest is installed
          pip install pytest pytest-cov

      - name: Run Tests with Auto-Fix Retry Logic
        id: tests_with_retry
        continue-on-error: true
        run: |
          MAX_RETRIES=3
          RETRY_COUNT=0
          TEST_PASSED=false
          
          echo "Starting tests with auto-fix retry logic (max $MAX_RETRIES attempts)"
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo "========================================="
            echo "Test Attempt $(($RETRY_COUNT + 1)) of $MAX_RETRIES"
            echo "========================================="
            
            # Run tests
            if $TEST_CMD; then
              echo "âœ… Tests passed on attempt $(($RETRY_COUNT + 1))"
              TEST_PASSED=true
              break
            else
              echo "âŒ Tests failed on attempt $(($RETRY_COUNT + 1))"
              RETRY_COUNT=$(($RETRY_COUNT + 1))
              
              # If not the last attempt, try to auto-fix
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "Attempting to analyze and fix test failures..."
                
                # Capture test output for analysis
                $TEST_CMD 2>&1 | tee test_output_attempt_${RETRY_COUNT}.log || true
                
                # TODO: Integrate with LLM or GitHub Copilot to analyze test failures
                # and suggest fixes. For now, we log the failure for manual review.
                echo "Test failure logged to test_output_attempt_${RETRY_COUNT}.log"
                
                # Placeholder for auto-fix logic
                # In production, this would:
                # 1. Analyze test output
                # 2. Use LLM/Copilot to generate fixes
                # 3. Apply fixes to code
                # 4. Retry tests
                
                echo "Auto-fix attempt $(($RETRY_COUNT)) - Currently placeholder implementation"
                echo "Waiting 2 seconds before retry..."
                sleep 2
              fi
            fi
          done
          
          # Set environment variables for subsequent steps
          echo "TEST_PASSED=$TEST_PASSED" >> $GITHUB_ENV
          echo "RETRY_COUNT=$RETRY_COUNT" >> $GITHUB_ENV
          
          if [ "$TEST_PASSED" = "true" ]; then
            echo "Final result: Tests passed after $RETRY_COUNT attempt(s)"
            exit 0
          else
            echo "Final result: Tests failed after $MAX_RETRIES attempts"
            exit 1
          fi

      - name: Commit Changes
        if: always()
        run: |
          git add .
          
          # Create appropriate commit message based on test results
          if [ "$TEST_PASSED" = "true" ]; then
            COMMIT_MSG="ðŸ¤– Jarvis Auto-Code: $JARVIS_INTENT - $JARVIS_INSTRUCTION"
            if [ "$RETRY_COUNT" -gt 0 ]; then
              COMMIT_MSG="$COMMIT_MSG (fixed after $RETRY_COUNT attempt(s))"
            fi
          else
            COMMIT_MSG="ðŸ¤– Jarvis Auto-Code: $JARVIS_INTENT - $JARVIS_INSTRUCTION (needs review - tests failed after $RETRY_COUNT attempts)"
          fi
          
          git commit -m "$COMMIT_MSG" || {
            echo "No changes to commit"
            echo "HAS_CHANGES=false" >> $GITHUB_ENV
            exit 0
          }
          echo "HAS_CHANGES=true" >> $GITHUB_ENV

      - name: Push Branch
        if: env.HAS_CHANGES == 'true'
        run: |
          git push origin "$BRANCH_NAME"

      - name: Create Pull Request
        if: env.HAS_CHANGES == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Determine PR title and body based on test results
          if [ "$TEST_PASSED" = "true" ]; then
            PR_TITLE="ðŸ¤– Jarvis: $JARVIS_INTENT - $JARVIS_INSTRUCTION"
            
            if [ "$RETRY_COUNT" -eq 0 ]; then
              STATUS_MSG="âœ… All tests passed on first attempt"
            else
              STATUS_MSG="âœ… All tests passed after $RETRY_COUNT auto-fix attempt(s)"
            fi
            
            PR_BODY="## ðŸ¤– Jarvis Self-Healing Auto-Code

**Intent:** $JARVIS_INTENT
**Instruction:** $JARVIS_INSTRUCTION
**Context:** $JARVIS_CONTEXT

### âœ… Quality Check
- Tests executed: $TEST_CMD
- Result: $STATUS_MSG

### ðŸ“Š Workflow Details
- Triggered by: ${{ github.event.client_payload.triggered_by }}
- Workflow run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

---
*Generated automatically by Jarvis Self-Healing System*"
          else
            PR_TITLE="ðŸ”§ Jarvis: $JARVIS_INTENT - $JARVIS_INSTRUCTION (Needs Review)"
            
            PR_BODY="## ðŸ¤– Jarvis Self-Healing Auto-Code - Needs Manual Review

**Intent:** $JARVIS_INTENT
**Instruction:** $JARVIS_INSTRUCTION
**Context:** $JARVIS_CONTEXT

### âš ï¸ Quality Check - Needs Attention
- Tests executed: $TEST_CMD
- Result: âŒ Tests failed after $RETRY_COUNT auto-fix attempts
- Status: **Requires manual review and intervention**

### ðŸ“‹ Test Failure Report
The automated fix attempts were unsuccessful. Please review the workflow logs and test output files for detailed information:

- Workflow run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
- Test attempt logs: \`test_output_attempt_*.log\`

### ðŸ” Next Steps
1. Review the test failure logs in the workflow run
2. Analyze the root cause of the failures
3. Apply manual fixes or provide additional context to Jarvis
4. Re-run tests or trigger a new fix attempt

### ðŸ“Š Workflow Details
- Triggered by: ${{ github.event.client_payload.triggered_by }}
- Auto-fix attempts: $RETRY_COUNT
- Workflow run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

---
*Generated automatically by Jarvis Self-Healing System*"
          fi
          
          gh pr create \
            --title "$PR_TITLE" \
            --body "$PR_BODY" \
            --base "${{ github.event.repository.default_branch }}" \
            --head "$BRANCH_NAME" || {
              echo "PR might already exist, trying to update instead..."
              # If PR already exists, we could update it here
              true
            }

      - name: Workflow Summary
        if: always()
        run: |
          echo "## Jarvis Self-Healing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Intent:** $JARVIS_INTENT" >> $GITHUB_STEP_SUMMARY
          echo "**Instruction:** $JARVIS_INSTRUCTION" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "$TEST_PASSED" = "true" ] && [ "${HAS_CHANGES:-false}" = "true" ]; then
            if [ "$RETRY_COUNT" -eq 0 ]; then
              echo "âœ… **Status:** SUCCESS - Tests passed on first attempt, PR created" >> $GITHUB_STEP_SUMMARY
            else
              echo "âœ… **Status:** SUCCESS - Tests passed after $RETRY_COUNT auto-fix attempt(s), PR created" >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${HAS_CHANGES:-false}" = "true" ]; then
            echo "âš ï¸ **Status:** NEEDS REVIEW - Tests failed after $RETRY_COUNT attempts, PR created for manual review" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Action Required:** Please review the PR and test failure logs to determine next steps" >> $GITHUB_STEP_SUMMARY
          elif [ "${HAS_CHANGES:-false}" = "false" ]; then
            echo "â„¹ï¸ **Status:** No changes needed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Status:** FAILED - Unable to process request" >> $GITHUB_STEP_SUMMARY
          fi
